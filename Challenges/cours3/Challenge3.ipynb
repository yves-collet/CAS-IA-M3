{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf89f6c4-d472-4057-bd0a-61c58754b249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML Pipeline using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a75d1f88-8682-4584-b42f-08a91d1c40c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load dataset\n",
    "data_path = \"/Volumes/yvescollet_lakehouse/ml_sandbox/data/train.csv\"\n",
    "train_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Cast Boolean columns to int\n",
    "train_df = train_df.withColumn(\"PassengerId\", col(\"PassengerId\").cast(\"string\")) \\\n",
    "                   .withColumn(\"VIP\", col(\"VIP\").cast(\"int\")) \\\n",
    "                   .withColumn(\"CryoSleep\", col(\"CryoSleep\").cast(\"int\")) \\\n",
    "                   .withColumn(\"Transported\", col(\"Transported\").cast(\"int\")) \n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0482d56-3631-409f-acb6-494b6f227346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pandas & scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d27ed428-55cb-46ef-b915-c522eb99334d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = train_df.toPandas()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d302f6b0-c7f7-4a0b-b93a-40d8b451b782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Define transformers for different column types\n",
    "numerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\"))]\n",
    ")\n",
    "\n",
    "categorical_cols = ['HomePlanet', 'Destination', 'VIP', 'CryoSleep']\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Step 2: Create a ColumnTransformer that applies the transformations to the columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "# Step 3: Assemble the preprocessing pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "# Fit and transform the DataFrame\n",
    "X_preprocessed = preprocessing_pipeline.fit_transform(train)\n",
    "\n",
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6fe53945-d4f5-4c3a-8e0d-f4d194869005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converting back to Pandas DataFrame\n",
    "onehot_encoder_feature_names = list(preprocessing_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['encoder'].get_feature_names_out())\n",
    "column_order =  numerical_cols + onehot_encoder_feature_names\n",
    "\n",
    "# Show the cleaned DataFrame\n",
    "pd.DataFrame(X_preprocessed, columns=column_order, index=train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "139464e2-21cb-4c7f-aae6-58d46ec5a767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Decision Tree Classifier \n",
    "\n",
    "We extend the pipeline with a decision tree classifier to predict the Transported variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f2885672-bbcd-4994-8870-5a066fd17d18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = train.drop('Transported', axis=1)\n",
    "y = train['Transported']\n",
    "\n",
    "# Define the hyperparameters for the DecisionTreeClassifier\n",
    "hyperparams = {\n",
    "    'criterion': 'entropy',     # Function to measure the quality of a split\n",
    "    'max_depth': 3,             # Limits the depth of the tree to prevent overfitting\n",
    "    'min_samples_split': 20,    # The minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': 10,     # The minimum number of samples required to be at a leaf node\n",
    "    'random_state': 42          # Ensures reproducibility of the results\n",
    "}\n",
    "\n",
    "# Update the model pipeline with the new DecisionTreeClassifier parameters\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(**hyperparams))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model_pipeline.fit(X, y)\n",
    "\n",
    "model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74857865-fb19-4727-a1d3-7c44cfd384ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Extract the decision tree model\n",
    "decision_tree_model = model_pipeline.named_steps['classifier']\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(decision_tree_model, \n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          class_names=['Not Transported', 'Transported'],\n",
    "          feature_names=column_order)  # Ensure 'column_order' matches the order of features in the trained model\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dda3055-a13c-4049-902c-fd7a1275c49d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Loading test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c51f5046-be9b-497f-a39b-b166697eb745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/yvescollet_lakehouse/ml_sandbox/data/test.csv\"\n",
    "test_df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Cast Boolean columns to int\n",
    "test_df = test_df.withColumn(\"PassengerId\", col(\"PassengerId\").cast(\"string\")) \\\n",
    "                   .withColumn(\"VIP\", col(\"VIP\").cast(\"int\")) \\\n",
    "                   .withColumn(\"CryoSleep\", col(\"CryoSleep\").cast(\"int\"))\n",
    "\n",
    "test = test_df.toPandas()\n",
    "\n",
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7102a43b-ae41-457a-92ae-f6b6786d64f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_test = test\n",
    "\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "kaggle_submission = pd.DataFrame(y_pred, columns=['Transported'], index=X_test.index)\n",
    "kaggle_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf1fffa-fbd1-489d-9e02-80cb52d6357b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kaggle_submission.to_csv(\"/Volumes/yvescollet_lakehouse/ml_sandbox/data/simple_decision_tree.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Challenge3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
